This section presents the pain maps and the preprocessing. Furthermore, the multiple pain map representations are described, whereafter the linearity  of the pain maps is investigated. Finally, the deep learning architectures are presented.

\subsection*{\textbf{Pain maps}}
Pain maps used in this study were collected from an on-going clinical trial (FOXH) which is conducted in collaboration with Danish and Australian universities. The pain maps were drawn by individuals with PFP syndrome through the use of an application, Navigate Pain, in a clinical setting. \newline
\noindent
Navigate Pain is a software application that is used to visualize the location, morphology, and spatial distribution of pain from individuals to healthcare personnel. The application permits individuals to draw their pain with different colors and line thickness onto a body outline, an example is shown in fig. \ref{fig:twoPainmaps}. Navigate Pain was developed at Aalborg University.\citep{Solutions2015}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{Figures/twoPainmaps}
\caption{Pain maps from individuals with uni- and bilateral PFP. The red markings indicate the area of pain perceived by the individuals.}
\label{fig:twoPainmaps}
\end{figure}

\noindent
The number of pain maps associated with pain duration was 205, and 197 associated to pain intensity measured according the Visual Analog Scale (VAS). The gender was included as an input, because females may report a more intense and frequent pain than males \citep{Pieh2012}, with the intentions of making the deep learning models better at classifying the pain maps.

\subsection*{\textbf{Pre-processing}}
The pain maps were processed in MatLab v. R2017b, where the images were resized, since they were collected at different resolutions (screen sizes) and cropped to only include the knees. To create more pain maps a split body approach was used, where the two knees in the pain maps were separated into individual images. The left knee was then mirrored to resemble the right knee to minimize the variance in the images. The final pain maps had a pixelsize of $252 \times 118$.
By using split body approach it was assumable that the pain duration and pain intensity were identical for both knees if PFP was bilateral. This resulted in an increased total number of pain maps with gender and pain duration to 333, and pain maps with gender and pain intensity to 319, of which 15\% was used as test data, and therefore not used to optimize and train the deep learning models. \newline
\noindent
The models were designed to classify pain maps according to pain duration or pain intensity divided into two classes with intervals based on the extremes. The classification intervals were 0 to 12 months and 36 to 300 months for pain duration, and 0 to 4 and 8 to 10 for pain intensity measured on VAS. The reason for choosing extreme intervals, was to separate closely related patterns between the two classes.

\subsubsection*{\textbf{Morphology-representation}}
The original pain maps reflect the morphology of the pain, and did not require further processing than converting the pain maps to a matrix including gender and the output, pain duration or pain intensity. As a result of using only the extremes for classifying, the number of pain maps decrease to 236 for pain duration, and 196 for pain intensity.

\subsubsection*{\textbf{Pain location}} 
The knee was divided into regions based on the underlying anatomical structures, which may have a correlation to pain duration or pain intensity.
The locations were divided into 10 regions, which were inspired by Photographic Knee Pain Map (PKPM). The divisions were designed to categorize location of knee pain for diagnostic and research purposes.\citep{Elson2010} The knee regions are illustrated in fig. \ref{fig:atlas}.

\begin{figure} [H] 
\centering
\includegraphics[width=0.22\textwidth]{Figures/atlas}
\caption{The regions of the right knee (R1-R10).}
\label{fig:atlas}
\end{figure}

\noindent 
There are ten regions, where regions 1 and 3 represent the superior lateral and superior medial areas for patella. Region 2 refers to quadriceps tendon. The patella is divided into lateral and medial regions, which are regions 4 and 5. Regions 6 and 8 are lateral and medial joint line areas. Patella tendon is region 7 and the two last regions, 9 and 10, are tibia lateral and medial.\citep{Elson2010}

\subsubsection*{\textbf{Location-representation}} 
A simplified representation of the pain maps was created to investigate whether the location have patterns related to the pain duration or pain intensity. The location of the pain was reflected by the use of the defined knee regions (fig. \ref{fig:atlas}), where a 10 elements vector were created. Each value represented either an active region (1) or a not active region (0). The values were defined by using a threshold to determine whether a region was considered active in relation the amount of pain in the specific region. A threshold was required to increase the confidence of an active pain region by avoiding minimal contributions. Simultaneously, the threshold should not be too high to avoid excluding too many or big areas. The threshold indicated which minimal percentage of pain pixels, that should be present in a specific region before it was considered as active. The threshold was decided based on an analysis on five random pain maps, where threshold values of 0, 5, 10 and 15\% was compared. Based on the analysis, a 5\% threshold was chosen. As a result of using the extremes for classification, and adding the threshold value, the number of pain maps with pain duration decreased to 223, and the number of pain maps with pain intensity decreased to 186.  

\subsubsection*{\textbf{Combined-representation}} 
A combination of morphology and location of the pain was created based on components from MR and LR. The original pain maps were superimposed on the knee regions, which resulted in pain pixels reflecting the location with a number from 1 to 10. Before using the representation as input in the deep learning models, a one-hot encoding approach was used, which made it possible to separate categorical data into binary data \citep{Harris2012}. As a result, the 10 values did not have a relation to each other when analysed in the deep learning models. By classifying according to the extremes, the number of pain maps decreased to 234 for pain duration, and 194 for pain intensity.

\subsection*{\textbf{Nonlinearity in pain maps}} 
Given that PFP is subjective and multifactorial it is unlikely that the pain maps and pain duration or pain intensity are linearly correlated. In order to determine if there was a linear relationship, linear regressions were done on simple features reflecting the size of the pain and number of active pain regions. Linear regressions were made in MatLab, and composed correlations between; number of pain pixels and pain duration, number of pain pixels and pain intensity, number of active pain regions and pain duration, and number of active pain regions and pain intensity.

\begin{figure*} [t!]
%\begin{tcolorbox}[colframe=black!30!black, colback=white]
\centering
\includegraphics[width=1\textwidth]{Figures/models}
\caption{The architecture of the deep learning models including the morphology-representation before optimization. The models consisted of three convolutional, three max pooling, and four fully connected layers. The gender was included in the first fully connected layer. The boxes represented the output after each layer.}
\label{fig:models}
%\end{tcolorbox}
\end{figure*}

\subsection*{\textbf{Architecture of the deep learning models}}
Deep learning models were developed on a computer with 4x ‘‘Intel® Core™ i7‘‘ CPUs and one single GPU of type "Geforce GTX 970M", using the programming language Python v3.6.3. Libraries used was Keras v2.0.8 with a TensorFlow v1.3.0 backend.\newline
\noindent
Six deep learning models suitable to the three pain map representations and pain duration or pain intensity were created. The models used supervised learning, which is defined as a network learning to classify a given input corresponding to a specific output \citep{Goodfellow2016}. The models were designed differently according to each pain map representation and type of classification. The architecture of the models, before optimization, including the MR is illustrated on fig. \ref{fig:models}. The models classified MR and CR consisted of three convolutional layers, to which a max pooling layer was added after each convolutional layer, and followed by four fully connected layers, where gender was inserted as a secondary input for the models. The only difference in the models using the CR was the input image, which was represented a matrix with 10 layers. 
The models including the LR consisted of only four fully connected layers of the architecture shown in fig. \ref{fig:models}. 

\subsubsection*{\textbf{The convolutional layers}}
Convolutional Neural Networks (CNNs) is a type of neural network for processing data with a grid-like topology \citep{Goodfellow2016}. CNNs were used to the MR and CR because of its capability to perform highly according to image classification. The purpose of the convolutional layers was to recognize the features in the pain maps by taking the image and scan it, then split it up into feature maps.\citep{Goodfellow2016,LeCun1998} The architecture of the first convolutional layer consisted of a kernel size on $5 \times 5$, and 32 filters. The two following convolutional layers consisted of kernel sizes on $3 \times 3$, and 32 filters. 

\subsubsection*{\textbf{Max pooling layers}}
For the models containing convolutional layers, each convolution layer was followed by a max pooling layer, which is a typical structure of a convolutional network \citep{Goodfellow2016, LeCun2015}.
Max pooling layers are used to reduce the size of the dataset, while maintaining features from the feature maps. Given a reduction in the data, the computation speed may increase.\citep{Goodfellow2016,LeCun1998} 
Max pooling layers were defined after each convolutional layer, to which all have a kernel size of $2 \times 2$ with a stride of 2. From the kernel window the highest of the 4 values was extracted to next layer, and used further through the network. 

\subsubsection*{\textbf{Fully connected layer and output layer}}
The models consisted of four fully connected layers, where the first layer received a flattened version of the input. The notation for gender was included in the end of the array, which was used as input in the first fully connected layer with 32 nodes. Additionally, the second and third layers consisted of 32 nodes. The fourth fully connected layer, which also was the output layer, included a sigmoid activation function. 
This function operates with a single output, that saturates when its input is either extremely negative or extremely positive \citep{Goodfellow2016}. The single output refers to the number of classification intervals, pain duration below 12 month, and above 36 month, or pain intensity below 4, and above 8 on VAS. 

\subsubsection*{\textbf{ReLU activation function}}
The activation function, chosen for all hidden layers in the models, was Rectified Linear Unit (ReLu), which transforms the linear output to a nonlinear function by making all negative values to zero. The ReLu function still remains nearly linear, which means it can easily be optimized with gradient descent based methods. In modern neural networks, ReLU is recommended to use as a default activation function and could be defined as $g(x) = max\{0, x\}$.\citep{Goodfellow2016}

\subsubsection*{\textbf{Dropout algorithm}}
A dropout algorithm was implemented for the models in the first two hidden fully connected layers to reduce overfitting while training. The algorithm works by randomly dropping a specified faction of the nodes in the given layer, to which the nodes that drop, and changes between different nodes during the training \citep{Srivastava2014}. Dropout reduce the nodes’ ability for co-adaptation, where multiple nodes compute the same features. For the models the dropout fraction was set to 0.5 (50\%) based on a previous study by \citeauthor{Srivastava2014} \citep{Srivastava2014}, which found 0.5 as optimal for multiple range of networks.

\subsubsection*{\textbf{Back-propagation algorithm}}
Back-propagation was used for the learning process where the weights of the models were adjusted in order to reduce the error calculated between the predicted output, and the correct output \citep{Duda2000}. Back-propagation is based on gradient descents, which computes gradients from the output to the input, in order to minimize the overall output error as much as possible during the learning stage. 
After each pass of a minibatch, the inputs and weights were multiplied for each node summed with additional coefficient bias.\citep{LeCun1998, Hameed2016}
Afterwards, a loss was calculated based on a loss function for every input that passed through the network to make the adjustments on the parameters to reduce the loss. As training progressed, the loss should decrease as a result of the parameter updates, and improve the performance of the neural network.\citep{Goodfellow2016, LeCun2015, Duda2000} This learning process continued until optimal parameters with minimum error were reached.\citep{Hameed2016}

\subsection*{\textbf{Training}}
The models were trained and optimized with a structured grid search on the hyperparameters to help set the initial parameters for the models. These hyperparameters refer to learning rate, number of filters and nodes, and number of epochs with different batch sizes. Accuracy was used to determine the improvement of performance when testing the multiple parameters.
Further the pain maps was divided into three subsets, a training set consisting of 75\%, a validation set consisting of 10\%, and a test set of the remaining 15\%. A manual optimization was performed, using the validation set, by evaluating the development in loss, accuracy during training, and the general performance estimated from accuracy. After optimization, the models were tested with the test set, using the hyperparameters from the optimization.
 

%\begin{figure*} [b!]
%\begin{tcolorbox}[colframe=black!30!black, colback=white]
%\centering
%\includegraphics[width=0.2\textwidth]{Figures/atlas}
%\caption{SÅDAN LAVER VI EN BOXER MED ET BILLED}
%\label{fig:atlas1}
%\end{tcolorbox}
%\end{figure*}



%\begin{figure*} [b!]
%\begin{tcolorbox}[colframe=black!30!black, colback=white]
%  \begin{subfigure}[b]{0.45\textwidth}
%    \includegraphics[width=\textwidth]{Figures/atlas}
%    \caption{FUCK JA}
%    \label{fig:f11}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[b]{0.45\textwidth}
%    \includegraphics[width=\textwidth]{Figures/twopainmaps}
%    \caption{FUCK NEJ}
%    \label{fig:f22}
%  \end{subfigure}
%  \caption{SÅDAN SÆTTER VI TO BILLEDER I EN}
%\end{tcolorbox}
%\end{figure*}