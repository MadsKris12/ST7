\textbf{Data and manual data handling} \newline
Data used in this study were collected beforehand from an on-going clinical trial (FOXH) which is conducted in collaboration with Danish and Australian universities. The data consists of pain maps which were drawn by individuals with PFPS through the use of an application, Navigate Pain, in a clinical setting. The pain maps are both from individuals with uni- and bilateral PFP, an example of these are shown in fig. \ref{fig:twoPainmaps}.

\begin{figure}[H]
\centering
\includegraphics[width=0.35\textwidth]{Figures/twoPainmaps}
\caption{Pain maps from individuals with uni- and bilateral PFP. The red markings indicate the area of pain perceived by the individuals.}
\label{fig:twoPainmaps}
\end{figure}

\noindent
In addition to the pain maps related information regarding the individuals was available.
Before using the data in the deep learning models, a manual data handling was necessary to match the given pain maps and associated ID on the individuals, which resulted in 217 available pain maps. Furthermore, specific information like gender, pain duration and intensity were collected from the appurtenant information. The number of pain maps with associated gender and pain duration, was 205. Furthermore, there were 197 pain maps with associated gender and pain intensity.\\

\noindent
\textbf{Software application: Navigate Pain} \newline
Navigate Pain is a software application that is used to visualise the location, morphology and spatial distribution of pain from individuals to healthcare personnel. The application permits individuals to draw their pain with different colors and line thickness onto a body outline. Navigate Pain android was developed at Aalborg University and a commercial web application is available at Aglance Solutions (Denmark).\citep{Solutions2015}\\

\noindent
\textbf{Knee regions} \newline
\noindent
To define the location of the PFP the knees are divided into 20 regions, which are inspired by Photographic Knee Pain Map (PKPM). The divisions are designed to categorise location of knee pain for diagnostic and research purposes. PKPM represent both knees that makes it possible to identify unilateral and bilateral pain.\citep{Elson2010} The knee regions are illustrated in fig. \ref{fig:atlas}.

\begin{figure} [H] 
\centering
\includegraphics[width=0.4\textwidth]{Figures/atlas}
\caption{The regions of the left (L1-L10) and right (R1-R10) knees, where each knee is split into ten regions.}
\label{fig:atlas}
\end{figure}

\noindent
The regions are based on the anatomical structures according to the areas where individuals often indicate pain.
There are ten regions on each knee, where region 1 and 3 represent the superior lateral and superior medial areas for patella. Region 2 refers to quadriceps tendon. The patella is divided into lateral and medial regions, which are region 4 and 5. Region 6 and 8 are lateral and medial joint line areas. Patella tendor is region 7 and the two last regions, 9 and 10, are tibia lateral and medial.\citep{Elson2010}\\

\noindent
\textbf{Data representations} \newline
\noindent
To investigate whether morphology and location of pain have an influence on the outputs, pain duration and intensity, the pain maps are encoded in multiple data representations. The pain maps were processed in MatLab, where the images were resized, since they were collected at different resolutions (screen sizes) and cropped to sort out unnecessary data like the areas inferior and superior to the knee.  Each data representation is reflected in a matrix consisting of the pain maps, gender and the output, pain duration and intensity. Since the original pain maps reflecting the morphology of the pain, thus the morphology-representation does not require further manipulation. \newline

\noindent
To investigate whether the location alone have a correlation to the outputs, a simplified representation of the pain maps are created. The location of the pain is then reflected by the use of the defined knee regions (fig. \ref{fig:atlas}), where each region represent a value of 0 (not active) or 1 (active) in a vector.  The values were defined by using a threshold to determine whether a region was considered active in relation the amount of pain. A threshold was required to increase the confidence of an active pain region by avoiding minimal contributions e.g. small pain areas in the associated regions. Simultaneously the threshold should not be too large so that pain areas was excluded. The threshold was decided based on an analysis on five random pain maps, where threshold values of 0, 5, 10 and 15\% was compared. The threshold represent which minimal percentage of pain should be present in a specific region before it is considered active. Based on the analysis a 5\% threshold was chosen. \newline
\noindent
Lastly, a data representation which reflects a combination of morphology and location of the pain, is prepared to explore if the interaction of morphology and location of pain would give a better classification according to the outputs. \newline

\noindent
\textbf{Linear regressions} \newline
\noindent
It was assumed that the data was nonlinear, because PFP is subjective and multidimensional. To verify this assumption of nonlinearity, linear regression on simple features reflecting the size of the pain was investigated. This decision was based on the phenomenon central sensitization that may result in widespread, to which this might be reflected in the number of pain pixels and active pain regions. 
Additionally, is a linear correlation according to the pain intensity investigated. The linear regression is shown in fig. \ref{fig:correlations}.



\begin{figure*} [t!]
\begin{tcolorbox}[colframe=black!30!black, colback=white]
\hfill
\begin{subfigure}[r]{0.5\textwidth}
    \includegraphics[width=\textwidth]{Figures/legend}
  \end{subfigure}
  \vskip\baselineskip
  \hspace{-5mm}
  \begin{subfigure}[b]{0.51\textwidth}
    \includegraphics[width=\textwidth]{Figures/durapixel}
    \caption{ }
    \label{fig:1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.51\textwidth}
    \includegraphics[width=\textwidth]{Figures/linRegDurationRegions}
       \caption{ }
    \label{fig:2}
  \end{subfigure}
    \vskip\baselineskip
    \hspace{-5mm}
  \begin{subfigure}[b]{0.51\textwidth}
    \includegraphics[width=\textwidth]{Figures/vaspixel}
    \caption{}
    \label{fig:3}
  \end{subfigure}
  \hfill
  \hspace{2mm}
  \begin{subfigure}[b]{0.51\textwidth}
    \includegraphics[width=\textwidth]{Figures/vasregion}
       \caption{ }
    \label{fig:4}
  \end{subfigure}  
  \caption{Linear correlations of pain pixels and pain duration (a), active pain regions and pain duration (b), pain pixels and pain intensity indicated in VAS (c), and active pain regions and pain intensity indicated in VAS (d).}
  \label{fig:correlations}
\end{tcolorbox}
\end{figure*}

\noindent
Based on the four linear regression models, it was shown that single features, number of pain pixels or number of active pain regions, did not have a clear linear correlation with the outputs, pain duration or intensity. Hence a deep learning model may find patterns in the pain maps according morphology or location of pain in relation to either pain duration or intensity. \newpage

\noindent
\textbf{Deep learning models}\newline
\noindent
Deep learning models were developed on a computer with 4x ‘‘Intel® Core™ i7‘‘ CPUs and one single GPU of type "Geforce GTX 970M", using the programming language Python v3.6.3. Libraries used was Keras with a TensorFlow backend. \newline
\noindent
Multiple deep learning models suitable to the three data representation were created. The models used supervised learning, which is defined as a network learning to classify a given input corresponding to a specific output \citep{Goodfellow2016}. The models classify the input, pain maps and gender, in relation to the determined outputs, pain duration or pain intensity.\newline
\noindent
Two of the models, which managed the morphology-, and the combined morphology and location-representations, were developed using the same model architecture consisting of convolutional- followed by pooling layers and fully connected layers. Convolutional were used because it’s highly classification in images that automatically learn a complex pattern by extracting visual features from the pixel-level content \citep{Acquarelli2017,LeCun1998}. The combination of convolutional and pooling layers performed feature extraction while the classification was made by fully connected layers. \newline
\noindent
The model that classified the location should not process morphology, thus a convolutional layer was not necessary, and thereby only contained fully connected layers. \\


\noindent
\textbf{Optimization of models}\newline
\noindent
Optimization of the three model were done using a validation subset, whereto graphs plotting validation accuracy and loss were compared with training accuracy and loss. This was used to estimate the optimal number of epochs to reduce overfitting the model to the training subset. Further optimization were done using manual search on hyperparameters, from which improvements were based on an average accuracy, sensitivity and specificity gained from 10-fold cross validation.  

%\begin{figure*} [b!]
%\begin{tcolorbox}[colframe=black!30!black, colback=white]
%\centering
%\includegraphics[width=0.2\textwidth]{Figures/atlas}
%\caption{SÅDAN LAVER VI EN BOXER MED ET BILLED}
%\label{fig:atlas1}
%\end{tcolorbox}
%\end{figure*}



%\begin{figure*} [b!]
%\begin{tcolorbox}[colframe=black!30!black, colback=white]
%  \begin{subfigure}[b]{0.45\textwidth}
%    \includegraphics[width=\textwidth]{Figures/atlas}
%    \caption{FUCK JA}
%    \label{fig:f11}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[b]{0.45\textwidth}
%    \includegraphics[width=\textwidth]{Figures/twopainmaps}
%    \caption{FUCK NEJ}
%    \label{fig:f22}
%  \end{subfigure}
%  \caption{SÅDAN SÆTTER VI TO BILLEDER I EN}
%\end{tcolorbox}
%\end{figure*}

