@misc{Nvidia,
author = {Nvidia},
title = {{GPU specs}},
url = {https://www.geforce.com/hardware/notebook-gpus/geforce-gtx-970m/specifications}
}
@misc{Intel,
author = {Intel},
title = {{Processor specs}},
url = {https://ark.intel.com/products/88967/Intel-Core-i7-6700HQ-Processor-6M-Cache-up-to-3{\_}50-GHz}
}
@article{Pieh2012,
author = {Pieh, Christoph and Altmeppen, Jurgen and Et.al},
doi = {10.1016/j.pain.2011.10.016},
journal = {Elsevier},
title = {{Gender differences in outcomes of a multimodal pain management program}},
year = {2012}
}
@article{Gustafson1998,
author = {Gustafson, Karl},
title = {{Assigning Initial Weights in Feedforward Neural Networks}},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017419380},
year = {1998}
}
@article{Mayer2011,
author = {Mayer, Tom G. and Neblett, Randy and Et.al},
doi = {10.1111/j.1533-2500.2011.00493.x},
journal = {World Institute of Pain},
title = {{The development and psychometric validation of the central sensitization inventory}},
year = {2011}
}
@article{Oesteraas2010,
author = {Oesteraas, Haavard and Torsensen, Tom Arild and Et.al},
doi = {10.3109/14038191003642385},
journal = {Advances in Physiotherapy},
number = {12:2},
pages = {95--99},
title = {{Treatment of chronic bilateral knee pain without objective clinical findings: Central sensitization. A case report}},
year = {2010}
}
@article{Nijs2011,
author = {Nijs, Jo and van Wilgen, Paul and Et.al},
doi = {10.1016/j.math.2011.04.005},
journal = {Elsevier},
title = {{How to explain central sensitization to patients with 'unexplained' chronic musculoskeletal pain: Practice guidelines}},
year = {2011}
}
@article{Matre2012,
author = {Matre, Dagfinn and Knardahl, Stein},
doi = {10.1016/j.sjpain.2012.04.003},
journal = {Scandinavian journal of pain},
title = {{'Central sensitization' in chronic neck/shoulder pain}},
year = {2012}
}
@article{Yunus2007,
author = {Yunus, Muhammed B.},
doi = {10.1016/j.berh.2017.03.006},
journal = {Elsevier},
title = {{Role of central sensitization in symptoms beyond muscle pain, and the evaluation of a patient with widespread pain}},
year = {2007}
}
@article{Dansie2013,
author = {Dansie, E. J. and Turk, D. C.},
doi = {10.1093/bja/aet124},
title = {{Assessment of patients with chronic pain}},
year = {2013}
}
@article{Abadi2016a,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Et.al},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
isbn = {0010-0277},
issn = {0270-6474},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
doi = {10.1051/0004-6361/201527329},
eprint = {1603.07285},
isbn = {9783319105895},
issn = {16113349},
pmid = {26353135},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@misc{KOOS2016,
author = {KOOS},
title = {{KOOS}},
url = {http://www.koos.nu/index.html},
year = {2016}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Et.al},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
volume = {86},
year = {1998}
}
@article{Patacchiola2017,
abstract = {{\textcopyright} 2017 Elsevier Ltd Head pose estimation is an old problem that is recently receiving new attention because of possible applications in human-robot interaction, augmented reality and driving assistance. However, most of the existing work has been tested in controlled environments and is not robust enough for real-world applications. In order to handle these limitations we propose an approach based on Convolutional Neural Networks (CNNs) supplemented with the most recent techniques adopted from the deep learning community. We evaluate the performance of four architectures on recently released in-the-wild datasets. Moreover, we investigate the use of dropout and adaptive gradient methods giving a contribution to their ongoing validation. The results show that joining CNNs and adaptive gradient methods leads to the state-of-the-art in unconstrained head pose estimation.},
author = {Patacchiola, M. and Cangelosi, A.},
doi = {10.1016/j.patcog.2017.06.009},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Adaptive gradient,Convolutional Neural Networks,Deep learning,Head pose estimation},
title = {{Head pose estimation in the wild using Convolutional Neural Networks and adaptive gradient methods}},
url = {https://ac.els-cdn.com/S0031320317302327/1-s2.0-S0031320317302327-main.pdf?{\_}tid=a09e6e8a-d356-11e7-9b28-00000aacb360{\&}acdnat=1511775647{\_}f91c81864b46ddf0becfc12de21bce48},
volume = {71},
year = {2017}
}
@article{Zhang2014,
abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
archivePrefix = {arXiv},
arxivId = {1412.6651},
author = {Zhang, Sixin and Choromanska, Anna and Et.al},
eprint = {1412.6651},
issn = {10495258},
title = {{Deep learning with Elastic Averaging SGD}},
url = {http://arxiv.org/abs/1412.6651},
year = {2014}
}
@article{Raschka2016,
author = {Raschka, Sebastian},
title = {{Single-Layer Neural Networks and Gradient Descent}},
url = {http://sebastianraschka.com/Articles/2015{\_}singlelayer{\_}neurons.html},
year = {2016}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {International Conference on Learning Representations},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
doi = {10.1111/j.0006-341X.1999.00591.x},
eprint = {1609.04747},
issn = {0006341X},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Qian1999,
abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force eld. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
author = {Qian, Ning},
isbn = {1212543521},
journal = {Neural Networks: The Official Journal of the International Neural Network Society},
keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence]},
number = {12(1)},
pages = {145--151},
title = {{On the Momentum Term in Gradient Descent Learning Algorithms The Momentum Term in Gradient Descent}},
url = {https://www.sciencedirect.com/science/article/pii/S0893608098001166{\#}aep-bibliography-id16},
volume = {5213},
year = {1999}
}
@article{Int82016,
author = {Int8},
journal = {Www},
title = {{Optimization techniques comparison in Julia: SGD, Momentum, Adagrad, Adadelta, Adam}},
url = {http://int8.io/comparison-of-optimization-techniques-stochastic-gradient-descent-momentum-adagrad-and-adadelta/},
year = {2016}
}
@article{Witvrouw2014,
author = {Witvrouw, Erik and Callaghan, Michael J. and Et.al},
doi = {10.1136/bjsports-2014-093450},
title = {{Patellofemoral Pain: consensus statement from the 3rd International Patellofemoral Pain Research Retreat held in Vancouver, September 2013}},
year = {2014}
}
@article{Dye2001,
author = {Dye, Scott F},
journal = {Sports Medicine and Arthroscopy Review},
title = {{Patellofemoral Pain Current Concepts: An Overview}},
year = {2001}
}
@article{Rathleff2015,
author = {Rathleff, M. S. and Vicenzino, B. and Et.al},
doi = {10.1007/s40279-015-0364-1},
title = {{Patellofemoral Pain in Adolescents and adulthood: same same, but different?}},
year = {2015}
}
@article{Elson2010,
author = {Elson, D. W. and Jones, S. and Et.al},
doi = {10.1016/j.knee.2010.08.012},
title = {{The photographic knee pain map: Locating knee pain with an instrument developed for diagnostic, communcation and research purposes}},
year = {2010}
}
@article{Valente2011,
author = {{Ferreira Valente}, Maria and {Ribeiro Pais}, Jos{\'{e}} and Et.al},
doi = {10.1016/j.pain.2011.07.005},
title = {{Validity of four pain intensity rating scales}},
year = {2011}
}
@article{Haefeli2005,
author = {Haefeli, Mathias and Elfering, Achim},
doi = {10.1007/s00586-005-1044-x},
title = {{Pain assessment}},
year = {2005}
}
@book{Duda2000,
abstract = {Pattern recognition is the construction of algorithms to decode and recognize images or data patterns in so-called random data. It is a vital and growing field with applications in artifical intelligence, machine learing, data mining, speech recognition, bioinformatics, and computer vision.},
author = {Duda, Richard and Hart, Peter and Et.al},
edition = {Second Edi},
isbn = {9780471056690},
pages = {680},
title = {{Pattern Classification}},
year = {2000}
}
@article{Dahab2011,
author = {Dahab, Katherine Stabenow},
doi = {10.4135/9781412961165.n415},
journal = {Enccyclopedia of Sports Medicine},
title = {{Q angle}},
year = {2011}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and El.al},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://dl.acm.org/citation.cfm?id=2670313{\&}CFID=818407627{\&}CFTOKEN=74532044},
volume = {15},
year = {2014}
}
@misc{Chollet2015,
abstract = {Keras: The Python Deep Learning library You have just found Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU. Read the documentation at Keras.io. Keras is compatible with: Python 2.7-3.6.},
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Keras.Io},
title = {{Keras Documentation}},
url = {https://keras.io},
year = {2015}
}
@incollection{Harris2012,
author = {Harris, David Money and Harris, Sarah L.},
booktitle = {Digital design and computer architecture},
isbn = {9780123978165},
publisher = {Elsevier},
title = {{Sequential Logic Design}},
year = {2012}
}
@article{Roos2003,
author = {Roos, Ewa M. and Lohmander, L. Stefan},
journal = {Bio Med Central},
title = {{The knee injury and osteoarthritis outcome score (KOOS): from joint injury to osteoarthritis}},
year = {2003}
}
@incollection{Schmidt1989,
author = {Schmidt, R. F},
booktitle = {Human Physiology},
doi = {10.1007/978-3-642-73831-9_10},
isbn = {978-3-642-73831-9},
publisher = {Springer Berlin Heidelberg},
title = {{Nociception and Pain}},
year = {1989}
}
@incollection{Ghosh2010,
author = {Ghosh, Kanishka M. and Deehan, David J.},
booktitle = {Orthopaedic surgery: lower limb},
editor = {Wilkinson, Mark},
publisher = {Elsevier},
title = {{Soft tissue knee injuries}},
year = {2010}
}
@incollection{Schmidt2013,
author = {Schmidt, R. F},
booktitle = {Fundamentals of Sensory Physiology},
edition = {2},
isbn = {9783662011287},
publisher = {Springer Science {\&} Business Media},
title = {{Nociception and Pain}},
year = {2013}
}
@book{Swamynathan2017,
author = {Swamynathan, Manohar},
doi = {10.1007/978-1-4842-2866-1},
isbn = {978-1-4842-2865-4},
title = {{Mastering Machine Learning with Python in Six Steps}},
url = {http://link.springer.com/10.1007/978-1-4842-2866-1},
year = {2017}
}
@article{Petersen2013,
abstract = {Data regarding validity of clinical and radiographic findings in diagnosing patellofemoral pain syndrome are inconclusive. We prospectively assessed how sensitive and specific key patellofemoral physical examination tests are, and evaluated the prevalence of physical examination and radiographic findings. Sixty-one infantry soldiers with patellofemoral pain syndrome and 25 control subjects were evaluated. The sensitivity of the patellar tilt, active instability, patella alta, and apprehension tests was low (less than 50{\%}); specificity ranged between 72{\%} and 100{\%}. Although the prevalence of positive patellar tilt and active instability tests was significantly greater in subjects with patellofemoral pain syndrome, there were no significant differences between the groups in the results of the other two tests. Soldiers with patellofemoral pain syndrome presented with increased quadriceps angle, lateral and medial retinacular tenderness, patellofemoral crepitation, squinting patella, and reduced mobility of the patella. There were no differences between the groups in the prevalence of lower limb and foot posture alignment and knee effusion. Plain radiography showed increased patellar subluxation in soldiers with patellofemoral pain syndrome. Other radiographic measures (sulcus angle, Laurin angle, Merchant angle, and Insall-Salvati index) were similar in both groups. We provide evidence regarding the validity of clinical and radiographic features commonly used for diagnosing patellofemoral pain syndrome. Physical examinations were more useful than plain radiography.},
author = {Haim, Amir and Yaniv, Moshe and Et.al},
doi = {10.1007/s00167-013-2759-6},
issn = {0009-921X},
journal = {Knee Surg sports traumatol arthrose},
pages = {223--228},
pmid = {16788411},
title = {{Patellofemoral Pain Syndrome}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00003086-200610000-00041},
volume = {451},
year = {2006}
}
@article{Crossley2016,
author = {Crossley, Kay M. and Callaghan, Michael J. and Et.al},
doi = {10.1136/bjsports-2015-h3939rep},
title = {{Patellofemoral pain}},
year = {2016}
}
@article{Crossley2015,
author = {Crossley, Kay M. and Callaghan, Michael J. and Et.al},
doi = {10.1136/bmj.h3939},
title = {{Patellofemoral pain}},
year = {2015}
}
@article{Smith2015,
author = {Smith, T.O. and Drew, B.T. and Et.al},
doi = {10.1002/14651858.CD010513.pub2.},
title = {{Knee orthoses for treating patellofemoral pain syndrome (review)}},
year = {2015}
}
@article{Maclachlan2017,
author = {Maclachlan, Liam R. and Collins, Natalie J. and Et.al},
doi = {10.1136-bjsports-2016-096705},
title = {{The psychological features of patellofemoral pain: a systematic review}},
year = {2017}
}
@book{Mehryar2012,
author = {Mehryar, Mohri and Afshin, Rostamizadeh and Et.al},
isbn = {9780262018258},
title = {{Foundations of Machine Learning}},
url = {https://ebookcentral.proquest.com/lib/aalborguniv-ebooks/reader.action?docID=3339482{\&}ppg=17{\#}},
year = {2012}
}
@book{Bengio2012,
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8_26},
isbn = {978-3-642-35289-8},
pages = {437--478},
title = {{Neural Networks: Tricks of the Trade: Second Edition}},
url = {http://dx.doi.org/10.1007/978-3-642-35289-8{\_}26},
year = {2012}
}
@article{Acquarelli2017,
abstract = {In this work we show that convolutional neural networks (CNNs) can be efficiently used to classify vibrational spectroscopic data and identify important spectral regions. CNNs are the current state-of-the-art in image classification and speech recognition and can learn interpretable representations of the data. These characteristics make CNNs a good candidate for reducing the need for preprocessing and for highlighting important spectral regions, both of which are crucial steps in the analysis of vibrational spectroscopic data. Chemometric analysis of vibrational spectroscopic data often relies on preprocessing methods involving baseline correction, scatter correction and noise removal, which are applied to the spectra prior to model building. Preprocessing is a critical step because even in simple problems using ‘reasonable' preprocessing methods may decrease the performance of the final model. We develop a new CNN based method and provide an accompanying publicly available software. It is based on a simple CNN architecture with a single convolutional layer (a so-called shallow CNN). Our method outperforms standard classification algorithms used in chemometrics (e.g. PLS) in terms of accuracy when applied to non-preprocessed test data (86{\%} average accuracy compared to the 62{\%} achieved by PLS), and it achieves better performance even on preprocessed test data (96{\%} average accuracy compared to the 89{\%} achieved by PLS). For interpretability purposes, our method includes a procedure for finding important spectral regions, thereby facilitating qualitative interpretation of results.},
author = {Acquarelli, Jacopo and van Laarhoven, Twan and Et.al},
doi = {10.1016/j.aca.2016.12.010},
issn = {18734324},
journal = {Analytica Chimica Acta},
keywords = {Convolutional neural networks,Preprocessing,Vibrational spectroscopy},
pages = {22--31},
pmid = {28081811},
title = {{Convolutional neural networks for vibrational spectroscopic data analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0003267016314842},
volume = {954},
year = {2017}
}
@article{Hameed2016,
abstract = {In this paper, we propose a novel machine learning classifier by deriving a new adaptive momentum back-propagation (BP) artificial neural networks algorithm. The proposed algorithm is a modified version of the BP algorithm to improve its convergence behavior in both sides, accelerate the convergence process for accessing the optimum steady-state and minimizing the error misadjustment to improve the recognized patterns superiorly. This algorithm is controlled by the learning rate parameter which is dependent on the eigenvalues of the autocorrelation matrix of the input. It provides low error performance for the weights update. To discuss the performance measures of this proposed algorithm and the other supervised learning algorithms such as k-nearest neighbours (k-NN), Naive Bayes (NB), linear discriminant analysis (LDA), support vector machines (SVM), BP, and BP with adaptive momentum (PBPAM) have been compared in term of speed of convergence, Sum of Squared Error (SSE), and accuracy by implementing benchmark problem - XOR and seven datasets from UCI repository.},
author = {Hameed, Alaa Ali and Karlik, Bekir and Et.al},
doi = {10.1016/j.knosys.2016.10.001},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Adaptive momentum,Back-propagation,Neural network,Supervised learning},
pages = {79--87},
title = {{Back-propagation algorithm with variable adaptive momentum}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705116303811?via{\%}3Dihub},
volume = {114},
year = {2016}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Et.al},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distin-guished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using " local search " to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were em-ployed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v4},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v4},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
volume = {61},
year = {2015}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Et.al},
doi = {10.1038/nature14539},
journal = {Nature Insight Review},
pages = {436--444},
title = {{Deep Learning}},
url = {https://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf},
year = {2015}
}
@misc{Solutions2015,
author = {Solutions, Aglance},
title = {{Visual insight for clinical reasoning – Navigate Pain}},
url = {http://www.navigatepain.com/},
year = {2015}
}
@article{Jordan2015,
abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jordan, M. I. and Mitchell, T. M.},
doi = {10.1126/science.aaa8415},
eprint = {arXiv:1011.1669v3},
isbn = {0036-8075, 0036-8075},
issn = {0036-8075},
journal = {Science},
number = {6245},
pages = {255--260},
pmid = {26185243},
title = {{Machine learning: Trends, perspectives, and prospects}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa8415},
volume = {349},
year = {2015}
}
@book{Nielsen2010,
abstract = {Begrebet machine learning, som p{\aa} dansk vel mest passende kan gengives med ”automatisk l{\ae}ring”, er baseret p{\aa} det s{\ae}rlige omr{\aa}de inden for datalogisk forskning, der hedder algo- ritmer kombineret med statistisk analyse. Det g{\aa}r ud p{\aa} at lade computeren l{\ae}re at genkende m{\o}nstre ud fra eksempler og data og vel at m{\ae}rke l{\ae}re mere end eksemplerne selv: P{\aa} basis af data kan computeren ogs{\aa} l{\ae}re at generalisere til nye eksempler og tr{\ae}ffe s{\aa}kaldt intelligente beslutninger.},
author = {Nielsen, Mads},
booktitle = {Diku},
isbn = {978-87-981270-5-5},
pages = {92--103},
publisher = {Datalogisk Institut},
title = {{Den digitale revolution – fort{\ae}llinger fra datalogiens verden}},
year = {2010}
}
@article{Grunnesjo2006,
author = {Grunnesj{\"{o}}, Marie},
doi = {10.1186/1471-2474-7-65},
title = {{The course of pain drawings during a 10-week treatment period in patients with acute and sub-acute low back pain}},
year = {2006}
}
@article{Briggs2010,
author = {Briggs, Emma},
title = {{Understanding the experience and physiology of pain}},
year = {2010}
}
@misc{IASP2012,
author = {IASP},
title = {{IASP Taxonomy}},
year = {2012}
}
@book{Martini2012,
author = {Martini, Frederic H. and Nath, Judi L. and Et.al},
title = {{Anatomy {\&} Physology}},
year = {2012}
}
@article{Boudreau2017,
author = {Boudreau, Shellie A. and Kamavuako, E. N. and Et.al},
doi = {10.1186/s12891-017-1521-5},
title = {{Distribution and symmetrical patellofemoral pain patterns as revealed by high-resolution 3D body mapping: a cross-sectional study}},
year = {2017}
}
@article{Bayam2017,
author = {Bayam, Levent and Arumilli, Rajendra and Et.al},
doi = {10.1093/pm/pnw326},
title = {{Testing shoulder pain mapping}},
year = {2017}
}
@article{Younger2009,
author = {Younger, Jarred and Mackey, Sean},
title = {{Pain outcomes: A brief review of instruments and techniques}},
year = {2009}
}
@article{Boudreau2016,
author = {Boudreau, Shellie A. and Badsberg, Susanne and Et.al},
doi = {10.1097/AJP.0000000000000230},
title = {{Digital pain drawings: Assessing Touch-Screen Technology and 3D Body Schemas}},
year = {2016}
}
@article{Schott2010,
author = {Schott, Geoffrey D.},
doi = {10.1016/j.ejpain.2009.12.005},
title = {{The cartography of pain: The evolving contribution of pain maps}},
year = {2010}
}
