@article{Valente2011,
author = {{Ferreira Valente}, Maria and {Ribeiro Pais}, Jos{\'{e}} and Al, Et.},
doi = {10.1016/j.pain.2011.07.005},
title = {{Validity of four pain intensity rating scales}},
year = {2011}
}
@article{Haefeli2005,
author = {Haefeli, Mathias and Elfering, Achim},
doi = {10.1007/s00586-005-1044-x},
title = {{Pain assessment}},
year = {2005}
}
@book{Duda2000,
abstract = {Pattern recognition is the construction of algorithms to decode and recognize images or data patterns in so-called random data. It is a vital and growing field with applications in artifical intelligence, machine learing, data mining, speech recognition, bioinformatics, and computer vision.},
author = {Duda, Richard and Hart, Peter and Stork, David},
edition = {Second Edi},
isbn = {9780471056690},
pages = {680},
title = {{Pattern Classification}},
year = {2000}
}
@article{Dahab2011,
author = {Dahab, Katherine Stabenow},
doi = {10.4135/9781412961165.n415},
journal = {Enccyclopedia of Sports Medicine},
title = {{Q angle}},
year = {2011}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://dl.acm.org/citation.cfm?id=2670313{\&}CFID=818407627{\&}CFTOKEN=74532044},
volume = {15},
year = {2014}
}
@misc{Chollet2015,
abstract = {Keras: The Python Deep Learning library You have just found Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU. Read the documentation at Keras.io. Keras is compatible with: Python 2.7-3.6.},
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Keras.Io},
title = {{Keras Documentation}},
url = {https://keras.io},
year = {2015}
}
@incollection{Harris2012,
author = {Harris, David Money and Harris, Sarah L.},
booktitle = {Digital design and computer architecture},
isbn = {9780123978165},
publisher = {Elsevier},
title = {{Sequential Logic Design}},
year = {2012}
}
@article{Roos2003,
author = {Roos, Ewa M. and Lohmander, L. Stefan},
journal = {Bio Med Central},
title = {{The knee injury and osteoarthritis outcome score (KOOS): from joint injury to osteoarthritis}},
year = {2003}
}
@incollection{Schmidt1989,
author = {Schmidt, R. F},
booktitle = {Human Physiology},
doi = {10.1007/978-3-642-73831-9_10},
isbn = {978-3-642-73831-9},
publisher = {Springer Berlin Heidelberg},
title = {{Nociception and Pain}},
year = {1989}
}
@incollection{Ghosh2010,
author = {Ghosh, Kanishka M. and Deehan, David J.},
booktitle = {Orthopaedic surgery: lower limb},
editor = {Wilkinson, Mark},
publisher = {Elsevier},
title = {{Soft tissue knee injuries}},
year = {2010}
}
@incollection{Schmidt2013,
author = {Schmidt, R. F},
booktitle = {Fundamentals of Sensory Physiology},
edition = {2},
isbn = {9783662011287},
publisher = {Springer Science {\&} Business Media},
title = {{Nociception and Pain}},
year = {2013}
}
@book{Swamynathan2017,
author = {Swamynathan, Manohar},
doi = {10.1007/978-1-4842-2866-1},
isbn = {978-1-4842-2865-4},
title = {{Mastering Machine Learning with Python in Six Steps}},
url = {http://link.springer.com/10.1007/978-1-4842-2866-1},
year = {2017}
}
@article{Petersen2013,
author = {Petersen, Wolf and Ellermann, Andree and Et.al.},
doi = {10.1097/01.blo.0000229284.45485.6c},
journal = {Clinical Orthopaedics and Related Research},
title = {{Patellofemoral Pain Syndrome}},
year = {2013}
}
@article{Crossley2016,
author = {Crossley, Kay M. and Callaghan, Michael J. and Et.al},
doi = {10.1136/bjsports-2015-h3939rep},
title = {{Patellofemoral pain}},
year = {2016}
}
@article{Crossley2015,
author = {Crossley, Kay M. and Callaghan, Michael J. and Et.al.},
doi = {10.1136/bmj.h3939},
title = {{Patellofemoral pain}},
year = {2015}
}
@article{Smith2015,
author = {Smith, T.O. and Drew, B.T. and Et.al.},
doi = {10.1002/14651858.CD010513.pub2.},
title = {{Knee orthoses for treating patellofemoral pain syndrome (review)}},
year = {2015}
}
@article{Maclachlan2017,
author = {Maclachlan, Liam R. and Collins, Natalie J. and Et.al},
doi = {10.1136-bjsports-2016-096705},
title = {{The psychological features of patellofemoral pain: a systematic review}},
year = {2017}
}
@book{Mehryar2012,
author = {Mehryar, Mohri and Afshin, Rostamizadeh and Ameet, Talwalka},
isbn = {9780262018258},
title = {{Foundations of Machine Learning}},
url = {https://ebookcentral.proquest.com/lib/aalborguniv-ebooks/reader.action?docID=3339482{\&}ppg=17{\#}},
year = {2012}
}
@book{Bengio2012,
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8_26},
isbn = {978-3-642-35289-8},
pages = {437--478},
title = {{Neural Networks: Tricks of the Trade: Second Edition}},
url = {http://dx.doi.org/10.1007/978-3-642-35289-8{\_}26},
year = {2012}
}
@article{Acquarelli2017,
abstract = {In this work we show that convolutional neural networks (CNNs) can be efficiently used to classify vibrational spectroscopic data and identify important spectral regions. CNNs are the current state-of-the-art in image classification and speech recognition and can learn interpretable representations of the data. These characteristics make CNNs a good candidate for reducing the need for preprocessing and for highlighting important spectral regions, both of which are crucial steps in the analysis of vibrational spectroscopic data. Chemometric analysis of vibrational spectroscopic data often relies on preprocessing methods involving baseline correction, scatter correction and noise removal, which are applied to the spectra prior to model building. Preprocessing is a critical step because even in simple problems using ‘reasonable' preprocessing methods may decrease the performance of the final model. We develop a new CNN based method and provide an accompanying publicly available software. It is based on a simple CNN architecture with a single convolutional layer (a so-called shallow CNN). Our method outperforms standard classification algorithms used in chemometrics (e.g. PLS) in terms of accuracy when applied to non-preprocessed test data (86{\%} average accuracy compared to the 62{\%} achieved by PLS), and it achieves better performance even on preprocessed test data (96{\%} average accuracy compared to the 89{\%} achieved by PLS). For interpretability purposes, our method includes a procedure for finding important spectral regions, thereby facilitating qualitative interpretation of results.},
author = {Acquarelli, Jacopo and van Laarhoven, Twan and Gerretzen, Jan and Tran, Thanh N. and Buydens, Lutgarde M.C. and Marchiori, Elena},
doi = {10.1016/j.aca.2016.12.010},
issn = {18734324},
journal = {Analytica Chimica Acta},
keywords = {Convolutional neural networks,Preprocessing,Vibrational spectroscopy},
pages = {22--31},
pmid = {28081811},
title = {{Convolutional neural networks for vibrational spectroscopic data analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0003267016314842},
volume = {954},
year = {2017}
}
@article{Hameed2016,
abstract = {In this paper, we propose a novel machine learning classifier by deriving a new adaptive momentum back-propagation (BP) artificial neural networks algorithm. The proposed algorithm is a modified version of the BP algorithm to improve its convergence behavior in both sides, accelerate the convergence process for accessing the optimum steady-state and minimizing the error misadjustment to improve the recognized patterns superiorly. This algorithm is controlled by the learning rate parameter which is dependent on the eigenvalues of the autocorrelation matrix of the input. It provides low error performance for the weights update. To discuss the performance measures of this proposed algorithm and the other supervised learning algorithms such as k-nearest neighbours (k-NN), Naive Bayes (NB), linear discriminant analysis (LDA), support vector machines (SVM), BP, and BP with adaptive momentum (PBPAM) have been compared in term of speed of convergence, Sum of Squared Error (SSE), and accuracy by implementing benchmark problem - XOR and seven datasets from UCI repository.},
author = {Hameed, Alaa Ali and Karlik, Bekir and Salman, Mohammad Shukri},
doi = {10.1016/j.knosys.2016.10.001},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Adaptive momentum,Back-propagation,Neural network,Supervised learning},
pages = {79--87},
title = {{Back-propagation algorithm with variable adaptive momentum}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705116303811?via{\%}3Dihub},
volume = {114},
year = {2016}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distin-guished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using " local search " to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were em-ployed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v4},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v4},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
volume = {61},
year = {2015}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
journal = {Nature Insight Review},
pages = {436--444},
title = {{Deep Learning}},
url = {https://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf},
year = {2015}
}
@misc{Solutions2015,
author = {Solutions, Aglance},
title = {{Visual insight for clinical reasoning – Navigate Pain}},
url = {http://www.navigatepain.com/},
year = {2015}
}
@article{Jordan2015,
abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jordan, M. I. and Mitchell, T. M.},
doi = {10.1126/science.aaa8415},
eprint = {arXiv:1011.1669v3},
isbn = {0036-8075, 0036-8075},
issn = {0036-8075},
journal = {Science},
number = {6245},
pages = {255--260},
pmid = {26185243},
title = {{Machine learning: Trends, perspectives, and prospects}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa8415},
volume = {349},
year = {2015}
}
@book{Nielsen2010,
abstract = {Begrebet machine learning, som p{\aa} dansk vel mest passende kan gengives med ”automatisk l{\ae}ring”, er baseret p{\aa} det s{\ae}rlige omr{\aa}de inden for datalogisk forskning, der hedder algo- ritmer kombineret med statistisk analyse. Det g{\aa}r ud p{\aa} at lade computeren l{\ae}re at genkende m{\o}nstre ud fra eksempler og data og vel at m{\ae}rke l{\ae}re mere end eksemplerne selv: P{\aa} basis af data kan computeren ogs{\aa} l{\ae}re at generalisere til nye eksempler og tr{\ae}ffe s{\aa}kaldt intelligente beslutninger.},
author = {Nielsen, Mads},
booktitle = {Diku},
isbn = {978-87-981270-5-5},
pages = {92--103},
publisher = {Datalogisk Institut},
title = {{Den digitale revolution – fort{\ae}llinger fra datalogiens verden}},
year = {2010}
}
@article{Grunnesjo2006,
author = {Grunnesj{\"{o}}, Marie},
doi = {10.1186/1471-2474-7-65},
title = {{The course of pain drawings during a 10-week treatment period in patients with acute and sub-acute low back pain}},
year = {2006}
}
@article{Briggs2010,
author = {Briggs, Emma},
title = {{Understanding the experience and physiology of pain}},
year = {2010}
}
@misc{IASP2012,
author = {IASP},
title = {{IASP Taxonomy}},
year = {2012}
}
@book{Martini2012,
author = {et. al. Martini, Frederic H.},
title = {{Anatomy {\&} Physology}},
year = {2012}
}
@article{Boudreau2017,
author = {Boudreau, Shellie A. and et.al. Kamavuako, E. N.},
doi = {10.1186/s12891-017-1521-5},
title = {{Distribution and symmetrical patellofemoral pain patterns as revealed by high-resolution 3D body mapping: a cross-sectional study}},
year = {2017}
}
@article{Bayam2017,
author = {Bayam, Levent and et.al. Arumilli, Rajendra},
doi = {10.1093/pm/pnw326},
title = {{Testing shoulder pain mapping}},
year = {2017}
}
@article{Younger2009,
author = {Younger, Jarred and Mackey, Sean},
title = {{Pain outcomes: A brief review of instruments and techniques}},
year = {2009}
}
@article{Boudreau2016,
author = {Boudreau, Shellie A. and et.al Badsberg, Susanne},
doi = {10.1097/AJP.0000000000000230},
title = {{Digital pain drawings: Assessing Touch-Screen Technology and 3D Body Schemas}},
year = {2016}
}
@article{Schott2010,
author = {Schott, Geoffrey D.},
doi = {10.1016/j.ejpain.2009.12.005},
title = {{The cartography of pain: The evolving contribution of pain maps}},
year = {2010}
}
